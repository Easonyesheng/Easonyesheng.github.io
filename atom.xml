<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>指北集</title>
  
  <subtitle>二十一世纪漫游指北</subtitle>
  <link href="http://northpointer.xyz/atom.xml" rel="self"/>
  
  <link href="http://northpointer.xyz/"/>
  <updated>2020-09-13T06:56:18.778Z</updated>
  <id>http://northpointer.xyz/</id>
  
  <author>
    <name>Eason Zhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SuperGlue 基于图神经网络和注意力机制的特征点匹配网络</title>
    <link href="http://northpointer.xyz/2020/09/07/SuperGlue/"/>
    <id>http://northpointer.xyz/2020/09/07/SuperGlue/</id>
    <published>2020-09-07T07:26:22.000Z</published>
    <updated>2020-09-13T06:56:18.778Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SuperGlue"><a href="#SuperGlue" class="headerlink" title="SuperGlue"></a>SuperGlue</h1><hr><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>SuperGlue是Magicleap团队在SuperPoint后的又一力作，其思路与方法之巧妙不输SuperPoint，完成的任务也是传统视觉问题中特征点提取和描述（SuperPoint）的后一步–特征点匹配。<br>说起匹配，最传统的就是直接根据上一步提取出的特征描述子，计算两者间的欧式距离来衡量匹配的概率，也就是俗称的最近邻搜索（NN Search）。最近几年，随着深度学习的发展，有学者将处理点云数据的网络PointNet应用到匹配任务中，如PointCN，OANet等，但这些都是直接将所有匹配点一股脑输入网络，用网络拟合函数做分类任务。而SuperGlue，则为匹配问题提供了一个新的思路。  </p><hr><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>SuperGlue对于匹配问题的新看法，是用一个经典的数学问题来定义，即最优运输问题（Optimal Transport）。  </p><h3 id="最优运输问题"><a href="#最优运输问题" class="headerlink" title="最优运输问题"></a>最优运输问题</h3><p>关于最优运输问题的一个例子就是：把一堆沙子里的每一铲都对应到一个沙雕上的一铲沙子，怎么搬沙子最省力气。数学上的解释就是：从一个概率分布转换到另一个概率分布所需要的最小代价。<br>那么匹配问题是如何看成最优运输的呢？我的理解是：<strong>原先没有匹配的点群作为一个概率分布，而匹配完成后的点是另一个概率分布，这其中的转换代价就是匹配概率。</strong><br>所以SuperGlue就是用神经网络来预测这种转换代价，也就是最终输出的分配矩阵（第i行第j列表示A图中第i个点与B图中第j个点之间的匹配概率）。<br>虽然是如此定义的，但在实际方法中，可以看到，SuperGLue先是从原始的特征点分布用图神经网络估计出转换到一个中间分布的（得分矩阵）cost，然后从中间分布到最终的分布又可以看作是一个OT问题，在这一步，作者则是直接采用了传统可微的Sinkhore算法来得到最终的分配矩阵。  </p><hr><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p><strong>SuperGlue的作者认为，一个好的匹配算法，应该是能够找到所有存在匹配的点的匹配点，同时还要甄别出那些无匹配的点，所以作者将最后的输出定义为一个分配矩阵P。</strong><br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/PaperNotes/SuperGlue_partial_assignment.png" alt="Magic"><br>如何得到这个分配矩阵呢？作者首先从人类视觉的角度做了一些思考。我们在找匹配的时候，首先需要确立一个点的特殊性，显而易见的，需要这个点的位置和像素信息；其次，我们还可以从一个点与另一个点的相对位置关系来确定其特殊性；最后，在另一张图中的信息也可以用于判别一些难以区分的情况（人为定义信息之间的拓扑结构）。同时，我们往往会在两张图片中来回对比（迭代），将注意力集中在某些点与点的关系上（attention）。<br>基于以上几点，作者提出了如下图所示的网络结构。首先，作者设计了一个encoder，用于将之前提取出的点的位置和描述整合到一起，然后用图的形式将图内和图间的特征点连接起来（相当于一种信息流向的先验知识），并且在图的边上增加了可学习的权重作为attention机制，通过图网络的迭代，最后得到特征点的匹配描述子（f）。<br>然后作者根据这个描述子对各点的匹配进行打分（用描述子的内积，来衡量相似度），得到一个匹配得分矩阵，而由于遮挡等因素，可能会出现无匹配的情况，作者就在这个得分矩阵的最后增加了一行一列（dustbin），用于放置无匹配的特征点。<br>从得分矩阵到最后的分配矩阵，可以看作是一个OT问题，作者用传统的可微的Sinkhorn算法来解决。最后drop dustbin后得到分配矩阵P。<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/PaperNotes/SuperGlue_archi.png" alt="Magic"><br>网络的loss采用分配矩阵中代表匹配的概率值的log形式。  </p><hr><h2 id="Experiment-amp-Result"><a href="#Experiment-amp-Result" class="headerlink" title="Experiment &amp; Result"></a>Experiment &amp; Result</h2><p>这篇文章的实验是挑选了两个以匹配为核心的视觉任务–单应估计、姿态估计。对比的对象是传统匹配算法（NN）和learning-based算法（OANet）。同时，作者还在前段提取算法上，做了一些组合，最后得到的结论就是SuperPoint+SuperGlue效果最好。<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/PaperNotes/SuperGlue_exp1.png" alt="Magic"><br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/PaperNotes/SuperGlue_exp2.png" alt="Magic">  </p><hr><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>SuperGlue这个方法给人第一直觉上是很合理的，不仅有数学做支撑，也有结合了信息流拓扑结构的网络，给人很make sense的感觉。作者也认为这是通往DeepSLAM的一个重要里程碑。  </p>]]></content>
    
    
    <summary type="html">A milestone towards end-to-end deep SLAM.</summary>
    
    
    
    <category term="博客" scheme="http://northpointer.xyz/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
    <category term="论文笔记" scheme="http://northpointer.xyz/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Superpoint 论文解读</title>
    <link href="http://northpointer.xyz/2020/08/28/Superpoint/"/>
    <id>http://northpointer.xyz/2020/08/28/Superpoint/</id>
    <published>2020-08-28T08:03:14.000Z</published>
    <updated>2020-08-31T08:29:08.131Z</updated>
    
    <content type="html"><![CDATA[<h1 id="方法论"><a href="#方法论" class="headerlink" title="方法论"></a>方法论</h1><p>对于一篇论文，应该用何种框架去解构？<br><strong>之于背景的目的、之于目前challenge的出发点、新的ideal、验证实验、不足和后续。</strong><br>对于Learning-based方法，又该如何弄清楚它？<br><strong>输入、输出、结构、loss、训练方法、metric</strong>  </p><hr><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Superpoint的作者Daniel DeTone来自Magicleap，一家做AR的独角兽。作者在Superpoint之前，还发过几篇相关的文章，可以看作是Superpoint的酝酿过程。第一篇是做单应矩阵估计的‘Deep image homography estimation’，这篇文章说是做单应估计，实际上就是回归图像边界的角点坐标，也正因为如此（猜测），作者注意到网络检测角点的可能性，在之后的工作中又提出了Magicpoint，一个专门用于interest points检测的网络，同时也是Superpoint的主要结构之一。这两个工作之后，作者又从SIFT算法中得到启发，最终提出了基于自监督的特征点提取算法（Superpoint）。</p><hr><h2 id="目的和出发点"><a href="#目的和出发点" class="headerlink" title="目的和出发点"></a>目的和出发点</h2><p>特征点是指图片中不随环境光和视角变化的点，在视觉任务中往往是作为前提的存在，因此从图片中提取特征点是计算机视觉中很基础的任务。随着目前神经网络的发展，特征点提取任务可以很自然地定义为有监督的学习任务，以人工标注为监督信息，通过训练最先进的网络来检测特征点就可以实现。<br>然而，作者认为对于语意特征点，比如人体姿态估计时的手肘、肢端等等，目前人为的特征点定义并不能体现其语意特征，进而会影响以这些点为监督信息训练的网络性能。（原文没有说清楚是怎么影响的）针对这一点，作者提出用神经网络自己检测出的点来作为监督信息训练网络，是所谓自监督。  </p><hr><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p>Superpoint的出发点是自监督，那么这些自监督信息从哪里来呢？这就要用到作者之前提出的Magicpoint，一种用于特征点提取的网络。  </p><h3 id="Magicpoint"><a href="#Magicpoint" class="headerlink" title="Magicpoint"></a>Magicpoint</h3><p><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/PaperNotes/Superpoint_Magicpoint.png" alt="Magic"><br>Magicpoint的输入是图片，输出是和图片相同size的heatmap，每个像素处的值是这个点作为interest point的概率，通过NMS（非极大值抑制）就可以得到最终的sparse interest points（稀疏特征点）。Magicpoint的结构是传统的encoder-decoder结构，其中encoder是VGG-style的网络，将输入边长降低到原始的1/8，同时增加65个通道，64个通道可以理解为原图中8x8的小patch，多出来一个通道是标注该patch有没有特征点的‘垃圾箱’,在decoder部分，先做通道间的softmax，然后舍弃掉softmax后概率最小的通道（舍弃多余信息），再做reshape（自像素卷积or像素洗牌），得到原始size的张量，经过NMS得到的结果就是稀疏特征点。<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/PaperNotes/Superpoint_Magicpoint_arch.png" alt="Magic"><br>Magicpoint采用的loss是分类的交叉熵。然后就是Magicpoint最特殊的一点–训练方法，Magicpoint用的是作者自己合成的数据集训练（虚拟的三维物体），其中包括线、立方体、三角形等等基础的几何结构，同时因为是合成的，所以真值特征点也有，为了增加泛化性，还在其中增加了高斯噪声和无特征点的圆形。<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/PaperNotes/Superpoint_Magicpoint_data.png" alt="Magic"><br>Magicpoint在虚拟数据集上取得了远胜于传统算法的结果，但是当应用于真实数据时，对于不同视角的图片，其检测出的特征点的可重复性不如传统算法，也就是换个视角，其之前能检测到的点就可能检测不到了。<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/PaperNotes/Superpoint_Magicpoint_res.png" alt="Magic">  </p><h3 id="Homographic-Adaptation"><a href="#Homographic-Adaptation" class="headerlink" title="Homographic Adaptation"></a>Homographic Adaptation</h3><p>针对Magicpoint在真实数据上的缺点，作者提出了Homographic Adaptation，也就是把真实图片做几次单应变换，将这些单应变换的结果都输入Magicpoint，然后将检测到的特征点投影到原始的图片上，合起来作为最后的特征点真值。这样使检测到的特征点更丰富，也具备了一定的单应不变性。<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/PaperNotes/Superpoint_HomoAda.png" alt="Magic">  </p><h3 id="Superpoint"><a href="#Superpoint" class="headerlink" title="Superpoint"></a>Superpoint</h3><p>Superpoint的输入是两张图片，其中一张是另一张单应变换得到的，为啥要用这样的两张输入呢，这就和它的输出有关了。类似于传统的SIFT算法，Superpoint的输出不仅仅是特征点还有特征点的描述子。由上可知，特征点是有真值的，而描述子没有，于是作者用两张单应变换的图片做输入（点的匹配是已知的），用这两张图片的特征点之间的匹配关系，来约束特征点描述子（具体看后面loss）。<br><em>关于什么是特征点的描述子，简单来说，描述子是用于不同图片特征点之间的匹配的，可以直接根据描述子间的欧氏距离来做匹配。因为特征点应该具有光、尺度等等的不变性，将图片做旋转、尺度、平移等等变换，然后对变换后的特征点求一定范围内的梯度，再在这些梯度中寻找共性，集合成一个矢量，就作为特征点的描述子。</em><br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/PaperNotes/Superpoint_io.png" alt="Magic"><br>Superpoint的结构也是encoder-decoder，encoder和Magicpoint相同，在decoder分为特征点和描述子的部分，特征点部分也和Magicpoint相同，在描述子部分，则是先学习半稠密的描述子（不使用稠密的方式是为了减少计算量和内存），然后进行双三次插值算法（bicubic interpolation）得到完整描述子，最后再使用L2标准化（L2-normalizes）得到单位长度的描述。<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/PaperNotes/Superpoint_archi.png" alt="Magic"><br>Superpoint的loss也分为特征点和特征描述子。<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/PaperNotes/Superpoint_loss_all.png" alt="Magic"><br>特征点的loss用的是全卷积交叉熵。<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/PaperNotes/Superpoint_loss_point.png" alt="Magic"><br>描述子部分则是用点之间的匹配情况来作为loss。<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/PaperNotes/Superpoint_loss_des.png" alt="Magic"><br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/PaperNotes/Superpoint_loss_match.png" alt="Magic"><br>最后作者的实验也验证了Superpoint在特征点检测和描述上的优势，其中描述子的准确性是用匹配的精度来衡量的。<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/PaperNotes/Superpoint_res.png" alt="Magic">  </p><hr><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Superpoint是作者Daniel DeTone一系列工作的总结。展现出神经网络对于特征点提取的强大能力，以及用另一个网络检测的结果作为监督信息的一种自监督的训练方式。  </p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>[1] D. DeTone, 2018, “Superpoint: Self-supervised interest point detection and description,” CVPR.  </li><li>[2] D. DeTone, “Toward Geometric Deep SLAM,” pp. 1–14, Jul. 2017.  </li><li>[3] D. DeTone, T. Malisiewicz, A. R. A. P. A. 1606.03798, 2016, “Deep image homography estimation,” arxiv.org  </li></ul>]]></content>
    
    
    <summary type="html">Learning-based Interest Point Detection and Description.</summary>
    
    
    
    <category term="博客" scheme="http://northpointer.xyz/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
    <category term="论文笔记" scheme="http://northpointer.xyz/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>YOLO V1 论文阅读笔记</title>
    <link href="http://northpointer.xyz/2020/08/19/Yolo-note/"/>
    <id>http://northpointer.xyz/2020/08/19/Yolo-note/</id>
    <published>2020-08-19T06:58:27.000Z</published>
    <updated>2020-08-22T03:45:23.220Z</updated>
    
    <content type="html"><![CDATA[<h1 id="YOLO-V1"><a href="#YOLO-V1" class="headerlink" title="YOLO V1"></a>YOLO V1</h1><h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>YOLO是You Only Look Once的简称，和人眼识别物体一样，看一眼就能识别出来，YOLO力求实现端到端的目标检测，只用一个简单的神经网络结构，从单张图片就可以输出图中目标的位置和目标的种类，而且因为其没有其他检测网络的复杂结构（说的就是你，Faster RCNN），所以其检测速度很快，可以实现实时的目标检测。<br><img src="https://img-blog.csdn.net/20180130221048856?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveGlhb2h1MjAyMg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="YOLO结构"></p><hr><h2 id="YOLO诞生之前-–-无火的时代"><a href="#YOLO诞生之前-–-无火的时代" class="headerlink" title="YOLO诞生之前 – 无火的时代"></a>YOLO诞生之前 – 无火的时代</h2><p><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/PaperNotes/sunshine.png" alt="赞美太阳"></p><center>Long May The Sunshine.</center>  在YOLO之前，目标检测的方法是从图像分类衍化而来，毕竟检测的第一步确实是将图片中的目标弄出来，而弄出来靠的就是先分类再定位，最后再对分出来的目标进行识别。  比较经典的是RCNN，利用区域推荐去估计目标位置（bonding box），再用卷积网络提取特征，作为后续SVM分类bonding box的依据，最后用线性模型调整box的位置，非极大值抑制（NMS）来去除重复的框。基于RCNN，还有提升了速度的Fast & Faster RCNN，所做的改进在于减少了重复计算以及将区域推荐也用网络来实现。  像RCNN这种具有复杂pipeline的目标检测方法自然是耗时冗长而无法用于实时检测的，尽管其精度很高，却也因此无法大规模用于实际场景。  <hr><h2 id="YOLO的诞生-–-传火的钟声响起"><a href="#YOLO的诞生-–-传火的钟声响起" class="headerlink" title="YOLO的诞生 – 传火的钟声响起"></a>YOLO的诞生 – 传火的钟声响起</h2><p>YOLO的目标很明确–实现实时的目标检测。而为了速度，就必须舍弃无效的动作，一击制敌。所以YOLO将目标检测做成了回归的形式，输入是图片，输出就是图片中目标的位置、种类，这么复杂的任务似乎也需要同样复杂的网络，但是YOLO只用了卷积层和全连接层，实现了天下武功，唯快不破。  </p><p><strong>而背后的原因是什么呢？众所周知，神经网络可以在问题和答案之间探索最佳的拟合方式，从而实现问题到答案的完美映射，取代了以往人工的算法设计；而使用神经网络的人呢，需要钻研的，就是问题的提问方式和答案的表示方式，以确保问题与答案相对的解空间中存在合理的拟合（人为定义的合理），以及探索如何指导神经网络朝正确的方向拟合过去和拟合的好坏（损失函数的设计）。</strong>  </p><p>那么对于YOLO来说，他的问题（输入）是一张图片，答案则是一个$(S \times S \times (5 \times B + C))$的张量。为什么这么设计答案？其合理性在于将原图分为S*S个小块，每个块内都有B个（论文中是2个，互相垂直）预测框（bonding box）的同一中心，每个框有5个参数$(x,y,w,h,confi)$，x，y是框的中心相对于小块中心的偏移，w，h是框的边长，congfi则是这个框预测结果的置信度，即预测框与真值框的IOU。同时对应于C个种类有C个预测值，如果一个目标的中心落在某个小块内，则这个块的框就负责该目标的预测。这些就构成了答案的维度。如下图所示。<br><img src="https://img-blog.csdnimg.cn/20190321161321184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjMyNTk4,size_16,color_FFFFFF,t_70" alt="YOLO示意图"><br><em>图片来自<a href="https://blog.csdn.net/qq_38232598/article/details/88695454?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param">传送门</a></em><br>如此定义，从问题到解空间就存在合理的映射，而如何指导网络学习这种映射，就涉及到损失函数的设计。<br>YOLO的损失函数采用的是平方和的形式，答案张量中的每一个部分都在损失函数中有所对应。如下图：<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/PaperNotes/YOLOV1_Loss.png" alt="loss"><br>loss的每个部分的对应如下：<br><img src="https://img-blog.csdnimg.cn/20190321203016680.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjMyNTk4,size_16,color_FFFFFF,t_70" alt="loss_explain"><br><em>图片来自<a href="https://blog.csdn.net/qq_38232598/article/details/88695454?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param">传送门</a></em><br>loss的第一部分是预测框的中心和大小，第二部分是bonding box的IOU，第三部分是分类误差。值得注意的是该loss虽然是所有块loss的总和，但是因为0-1系数的存在，只有当块负责某个目标时，才会有类别loss，只有当bbox对应于真值框时，才会有位置loss，这里的对应指的是bbox与真值的IOU最大。<br>为了平衡loss的各个部分，作者还加入了多个权重。比如让位置loss和类别loss同等重要是不合理的，毕竟是位置要优先于类别判断，所以在位置loss前增加了权重调节（论文中为5）。对于不存在目标的bbox的IOU loss，我们希望它的IOU越小越好，但因为大多数块都没有目标，所以在前面加入一个缩小权重的系数（论文中为0.5）。<br>而对于大小目标的检测来说，小目标检测出现的位置偏移误差更为难以忍受，所以在计算的时候，作者对于框的大小(w,h)开了方，因为在w，h越小的时候，相同的$\delta w, \delta h$，$\sqrt w, \sqrt h$展现的loss更大，这点很巧妙。  </p><hr><h2 id="YOLO的缺陷-–-阿克琉斯之踵"><a href="#YOLO的缺陷-–-阿克琉斯之踵" class="headerlink" title="YOLO的缺陷 – 阿克琉斯之踵"></a>YOLO的缺陷 – 阿克琉斯之踵</h2><p>YOLO V1因为机制的原因，存在的缺陷也很好理解。  </p><ul><li>无法检测大片的，靠近的目标，如鸟群。  </li><li>尽管用了平方根的方法，大小目标检测在loss中的差异不明显，导致位置预测仍是主要误差来源。  </li><li>输入是固定的，且因为下采样导致特征不精确。  </li></ul><hr><p>更多YOLO的实现细节，可以移步<a href="https://blog.csdn.net/qq_38232598/article/details/88695454?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param">传送门</a>  </p>]]></content>
    
    
    <summary type="html">You only look once。</summary>
    
    
    
    <category term="博客" scheme="http://northpointer.xyz/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
    <category term="论文笔记" scheme="http://northpointer.xyz/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Python实现的双目相机自标定系统</title>
    <link href="http://northpointer.xyz/2020/08/13/Python-%E5%8F%8C%E7%9B%AE%E8%87%AA%E6%A0%87%E5%AE%9A%E7%B3%BB%E7%BB%9F/"/>
    <id>http://northpointer.xyz/2020/08/13/Python-%E5%8F%8C%E7%9B%AE%E8%87%AA%E6%A0%87%E5%AE%9A%E7%B3%BB%E7%BB%9F/</id>
    <published>2020-08-13T07:48:08.000Z</published>
    <updated>2020-08-31T01:30:24.194Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python实现的双目相机自标定系统"><a href="#Python实现的双目相机自标定系统" class="headerlink" title="Python实现的双目相机自标定系统"></a>Python实现的双目相机自标定系统</h1><hr><h2 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h2><p>python-opencv 3.14<br>numpy<br>os<br>sys  </p><hr><h2 id="系统实现功能"><a href="#系统实现功能" class="headerlink" title="系统实现功能"></a>系统实现功能</h2><ul><li>读入图片和参数  </li><li>估计基础矩阵  </li><li>绘制对极线</li><li>分解得到本质矩阵和相机参数矩阵  </li><li>图片校正  </li></ul><p>下面是各个功能的核心代码。  </p><hr><h2 id="读入图片和参数"><a href="#读入图片和参数" class="headerlink" title="读入图片和参数"></a>读入图片和参数</h2><p>图片是使用常规的imread，重点是参数的读取。<br>这里选取的参数是kitti的标定文件，读取方式如下：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Paser</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Paser the calib_file </span></span><br><span class="line"><span class="string">    return a dictionary</span></span><br><span class="line"><span class="string">    use it as :</span></span><br><span class="line"><span class="string">        calib = self.Paser()</span></span><br><span class="line"><span class="string">        K1, K2 = self.calib[&#x27;K_0&#123;&#125;&#x27;.format(f_cam)], self.calib[&#x27;K_0&#123;&#125;&#x27;.format(t_cam)]</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    d = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> open(self.calib_path) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> f:</span><br><span class="line">            <span class="keyword">if</span> l.startswith(<span class="string">&quot;calib_time&quot;</span>):</span><br><span class="line">                d[<span class="string">&quot;calib_time&quot;</span>] = l[l.index(<span class="string">&quot;calib_time&quot;</span>)+<span class="number">1</span>:]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                [k,v] = l.split(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">                k,v = k.strip(), v.strip()</span><br><span class="line">                <span class="comment">#get the numbers out</span></span><br><span class="line">                v = [float(x) <span class="keyword">for</span> x <span class="keyword">in</span> v.strip().split(<span class="string">&quot; &quot;</span>)]</span><br><span class="line">                v = np.array(v)</span><br><span class="line">                <span class="keyword">if</span> len(v) == <span class="number">9</span>:</span><br><span class="line">                    v = v.reshape((<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">                <span class="keyword">elif</span> len(v) == <span class="number">3</span>:</span><br><span class="line">                    v = v.reshape((<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">                <span class="keyword">elif</span> len(v) == <span class="number">5</span>:</span><br><span class="line">                    v = v.reshape((<span class="number">5</span>,<span class="number">1</span>))</span><br><span class="line">                d[k] = v</span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure><hr><h2 id="估计基础矩阵"><a href="#估计基础矩阵" class="headerlink" title="估计基础矩阵"></a>估计基础矩阵</h2><p>基础矩阵的估计采用的是传统的算法，可选的是<strong>SIFT+RANSAC</strong>和<strong>SIFT+LMedS</strong>。<br>关键代码如下：  </p><p>提取特征点算法SIFT:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">sift = cv2.xfeatures2d.SIFT_create()</span><br><span class="line"></span><br><span class="line"><span class="comment"># find the keypoints and descriptors with SIFT</span></span><br><span class="line">kp1, des1 = sift.detectAndCompute(img1,<span class="literal">None</span>)</span><br><span class="line">kp2, des2 = sift.detectAndCompute(img2,<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># FLANN parameters</span></span><br><span class="line">FLANN_INDEX_KDTREE = <span class="number">0</span></span><br><span class="line">index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = <span class="number">5</span>)</span><br><span class="line">search_params = dict(checks=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">flann = cv2.FlannBasedMatcher(index_params,search_params)</span><br><span class="line">matches = flann.knnMatch(des1,des2,k=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">good = []</span><br><span class="line">pts1 = []</span><br><span class="line">pts2 = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># ratio test as per Lowe&#x27;s paper</span></span><br><span class="line"><span class="keyword">for</span> i,(m,n) <span class="keyword">in</span> enumerate(matches):</span><br><span class="line">    <span class="keyword">if</span> m.distance &lt; <span class="number">0.8</span>*n.distance:</span><br><span class="line">        good.append(m)</span><br><span class="line">        pts2.append(kp2[m.trainIdx].pt)</span><br><span class="line">        pts1.append(kp1[m.queryIdx].pt)</span><br><span class="line">pts1 = np.int32(pts1)</span><br><span class="line">pts2 = np.int32(pts2)</span><br><span class="line">F,mask = cv2.findFundamentalMat(pts1,pts2,cv2.FM_LMEDS)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mask 是之前的对应点中的内点的标注，为1则是内点。</span></span><br><span class="line"><span class="comment"># select the inlier points</span></span><br><span class="line">pts1 = pts1[mask.ravel() == <span class="number">1</span>]</span><br><span class="line">pts2 = pts2[mask.ravel() == <span class="number">1</span>]</span><br><span class="line">self.match_pts1 = np.int32(pts1)</span><br><span class="line">self.match_pts2 = np.int32(pts2)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>由坐标估计基础矩阵：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> method == <span class="string">&quot;RANSAC&quot;</span>:</span><br><span class="line">    self.FE, self.Fmask = cv2.findFundamentalMat(self.match_pts1,</span><br><span class="line">                                            self.match_pts2,</span><br><span class="line">                                            cv2.FM_RANSAC, <span class="number">0.1</span>, <span class="number">0.99</span>)</span><br><span class="line"><span class="keyword">elif</span> method == <span class="string">&quot;LMedS&quot;</span>:</span><br><span class="line">    self.FE, self.Fmask = cv2.findFundamentalMat(self.match_pts1,</span><br><span class="line">                                            self.match_pts2,</span><br><span class="line">                                            cv2.FM_LMEDS, <span class="number">0.1</span>, <span class="number">0.99</span>)</span><br></pre></td></tr></table></figure><p>对于基础矩阵估计的精度，有两个评价标准以及一个可视化的方法。<br><strong>Metrics</strong><br><strong>1.对极几何约束</strong><br>根据对极几何，基础矩阵和两张图的对应点之间满足以下公式：<br>$xFx’=0$<br>所以通过计算这个式子，可以大致判断基础矩阵的精度。<br><strong>2.对极线距离</strong><br>基础矩阵描述的是左图的点到右图对应极线的变换，而且右图的对应点也应该在这条极线上。<br>通过计算，点到极线的距离，就可以大致评价基础矩阵的精度。  </p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">EpipolarConstraint</span>(<span class="params">self,F,pts1,pts2</span>):</span></span><br><span class="line">       <span class="string">&#x27;&#x27;&#x27;Epipolar Constraint</span></span><br><span class="line"><span class="string">           calculate the epipolar constraint </span></span><br><span class="line"><span class="string">           x^T*F*x</span></span><br><span class="line"><span class="string">           :output </span></span><br><span class="line"><span class="string">               err_permatch</span></span><br><span class="line"><span class="string">       &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">       print(<span class="string">&#x27;Use &#x27;</span>,len(pts1),<span class="string">&#x27; points to calculate epipolar constraints.&#x27;</span>)</span><br><span class="line">       <span class="keyword">assert</span> len(pts1) == len(pts2)</span><br><span class="line">       err = <span class="number">0.0</span></span><br><span class="line">       <span class="keyword">for</span> p1, p2 <span class="keyword">in</span> zip(pts1, pts2):</span><br><span class="line">           hp1, hp2 = np.ones((<span class="number">3</span>,<span class="number">1</span>)), np.ones((<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">           hp1[:<span class="number">2</span>,<span class="number">0</span>], hp2[:<span class="number">2</span>,<span class="number">0</span>] = p1, p2</span><br><span class="line">           err += np.abs(np.dot(hp2.T, np.dot(F, hp1)))</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> err / float(len(pts1))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">SymEpiDis</span>(<span class="params">self, F, pts1, pts2</span>):</span></span><br><span class="line">       <span class="string">&quot;&quot;&quot;Symetric Epipolar distance</span></span><br><span class="line"><span class="string">           calculate the Symetric Epipolar distance</span></span><br><span class="line"><span class="string">       &quot;&quot;&quot;</span></span><br><span class="line">       epsilon = <span class="number">1e-5</span></span><br><span class="line">       <span class="keyword">assert</span> len(pts1) == len(pts2)</span><br><span class="line">       print(<span class="string">&#x27;Use &#x27;</span>,len(pts1),<span class="string">&#x27; points to calculate epipolar distance.&#x27;</span>)</span><br><span class="line">       err = <span class="number">0.</span></span><br><span class="line">       <span class="keyword">for</span> p1, p2 <span class="keyword">in</span> zip(pts1, pts2):</span><br><span class="line">           hp1, hp2 = np.ones((<span class="number">3</span>,<span class="number">1</span>)), np.ones((<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">           hp1[:<span class="number">2</span>,<span class="number">0</span>], hp2[:<span class="number">2</span>,<span class="number">0</span>] = p1, p2</span><br><span class="line">           fp, fq = np.dot(F, hp1), np.dot(F.T, hp2)</span><br><span class="line">           sym_jjt = <span class="number">1.</span>/(fp[<span class="number">0</span>]**<span class="number">2</span> + fp[<span class="number">1</span>]**<span class="number">2</span> + epsilon) + <span class="number">1.</span>/(fq[<span class="number">0</span>]**<span class="number">2</span> + fq[<span class="number">1</span>]**<span class="number">2</span> + epsilon)</span><br><span class="line">           err = err + ((np.dot(hp2.T, np.dot(F, hp1))**<span class="number">2</span>) * (sym_jjt + epsilon))</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> err / float(len(pts1))</span><br></pre></td></tr></table></figure><p><strong>可视化</strong><br>可视化是通过画出两张图上的对应点和极线来实现的。<br>因为极线会交于一点（极点），通过这个可以大致判断基础矩阵的精度。<br>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DrawEpipolarLines</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;For F Estimation visulization, drawing the epipolar lines</span></span><br><span class="line"><span class="string">        1. find epipolar lines</span></span><br><span class="line"><span class="string">        2. draw lines</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        self.FE.all()</span><br><span class="line">    <span class="keyword">except</span> AttributeError:</span><br><span class="line">        self.EstimateFM() <span class="comment"># use RANSAC as default</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        self.match_pts1.all()</span><br><span class="line">    <span class="keyword">except</span> AttributeError:</span><br><span class="line">        self.ExactGoodMatch()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Find epilines corresponding to points in right image (second image)</span></span><br><span class="line">    <span class="comment"># and drawing its lines on left image</span></span><br><span class="line">    pts2re = self.match_pts2.reshape(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    lines1 = cv2.computeCorrespondEpilines(pts2re, <span class="number">2</span>, self.FE)</span><br><span class="line">    lines1 = lines1.reshape(<span class="number">-1</span>, <span class="number">3</span>)</span><br><span class="line">    img3, img4 = self._draw_epipolar_lines_helper(self.imgl, self.imgr,</span><br><span class="line">                                                    lines1, self.match_pts1,</span><br><span class="line">                                                    self.match_pts2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Find epilines corresponding to points in left image (first image) and</span></span><br><span class="line">    <span class="comment"># drawing its lines on right image</span></span><br><span class="line">    pts1re = self.match_pts1.reshape(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    lines2 = cv2.computeCorrespondEpilines(pts1re, <span class="number">1</span>, self.FE)</span><br><span class="line">    lines2 = lines2.reshape(<span class="number">-1</span>, <span class="number">3</span>)</span><br><span class="line">    img1, img2 = self._draw_epipolar_lines_helper(self.imgr, self.imgl,</span><br><span class="line">                                                    lines2, self.match_pts2,</span><br><span class="line">                                                    self.match_pts1)</span><br><span class="line"></span><br><span class="line">    cv2.imwrite(os.path.join(self.SavePath,<span class="string">&quot;epipolarleft.jpg&quot;</span>),img1)</span><br><span class="line">    <span class="comment"># print(&quot;Saved in &quot;,os.path.join(self.SavePath,&quot;epipolarleft.jpg&quot;))</span></span><br><span class="line">    cv2.imwrite(os.path.join(self.SavePath,<span class="string">&quot;epipolarright.jpg&quot;</span>),img3)</span><br><span class="line"></span><br><span class="line">    cv2.startWindowThread()</span><br><span class="line">    cv2.imshow(<span class="string">&quot;left&quot;</span>, img1)</span><br><span class="line">    cv2.imshow(<span class="string">&quot;right&quot;</span>, img3)</span><br><span class="line">    k = cv2.waitKey()</span><br><span class="line">    <span class="keyword">if</span> k == <span class="number">27</span>:</span><br><span class="line">        cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure><hr><h2 id="由基础矩阵分解本质矩阵"><a href="#由基础矩阵分解本质矩阵" class="headerlink" title="由基础矩阵分解本质矩阵"></a>由基础矩阵分解本质矩阵</h2><p>这一步用到的公式是：$E = Kl\times F\times Kr$<br>代码如下：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_Get_Essential_Matrix</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Get Essential Matrix from Fundamental Matrix</span></span><br><span class="line"><span class="string">        E = Kl^T*F*Kr</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    self.E = self.Kl.T.dot(self.FE).dot(self.Kr)</span><br></pre></td></tr></table></figure><hr><h2 id="由本质矩阵分解得到相机矩阵-R-T"><a href="#由本质矩阵分解得到相机矩阵-R-T" class="headerlink" title="由本质矩阵分解得到相机矩阵[R|T]"></a>由本质矩阵分解得到相机矩阵[R|T]</h2><p>这里要用到SVD分解，并且要根据结果来测试四种可能性。具体原理可以去看<em>Multiple View Geometry in computer vision</em>.<br>代码如下：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_Get_R_T</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Get the [R|T] camera matrix</span></span><br><span class="line"><span class="string">        After geting the R,T, need to determine whether the points are in front of the images</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># decompose essential matrix into R, t (See Hartley and Zisserman 9.13)</span></span><br><span class="line">    U, S, Vt = np.linalg.svd(self.E)</span><br><span class="line">    W = np.array([<span class="number">0.0</span>, <span class="number">-1.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>,</span><br><span class="line">                    <span class="number">1.0</span>]).reshape(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># iterate over all point correspondences used in the estimation of the</span></span><br><span class="line">    <span class="comment"># fundamental matrix</span></span><br><span class="line">    first_inliers = []</span><br><span class="line">    second_inliers = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.Fmask)):</span><br><span class="line">        <span class="keyword">if</span> self.Fmask[i]:</span><br><span class="line">            <span class="comment"># normalize and homogenize the image coordinates</span></span><br><span class="line">            first_inliers.append(self.Kl_inv.dot([self.match_pts1[i][<span class="number">0</span>],</span><br><span class="line">                                    self.match_pts1[i][<span class="number">1</span>], <span class="number">1.0</span>]))</span><br><span class="line">            second_inliers.append(self.Kr_inv.dot([self.match_pts2[i][<span class="number">0</span>],</span><br><span class="line">                                    self.match_pts2[i][<span class="number">1</span>], <span class="number">1.0</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Determine the correct choice of second camera matrix</span></span><br><span class="line">    <span class="comment"># only in one of the four configurations will all the points be in</span></span><br><span class="line">    <span class="comment"># front of both cameras</span></span><br><span class="line">    <span class="comment"># First choice: R = U * Wt * Vt, T = +u_3 (See Hartley Zisserman 9.19)</span></span><br><span class="line">    R = U.dot(W).dot(Vt)</span><br><span class="line">    T = U[:, <span class="number">2</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self._in_front_of_both_cameras(first_inliers, second_inliers,</span><br><span class="line">                                            R, T):</span><br><span class="line">        <span class="comment"># Second choice: R = U * W * Vt, T = -u_3</span></span><br><span class="line">        T = - U[:, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self._in_front_of_both_cameras(first_inliers, second_inliers,</span><br><span class="line">                                            R, T):</span><br><span class="line">        <span class="comment"># Third choice: R = U * Wt * Vt, T = u_3</span></span><br><span class="line">        R = U.dot(W.T).dot(Vt)</span><br><span class="line">        T = U[:, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self._in_front_of_both_cameras(first_inliers,</span><br><span class="line">                                                second_inliers, R, T):</span><br><span class="line">            <span class="comment"># Fourth choice: R = U * Wt * Vt, T = -u_3</span></span><br><span class="line">            T = - U[:, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    self.match_inliers1 = first_inliers</span><br><span class="line">    self.match_inliers2 = second_inliers</span><br><span class="line">    self.Rt1 = np.hstack((np.eye(<span class="number">3</span>), np.zeros((<span class="number">3</span>, <span class="number">1</span>)))) <span class="comment"># as defaluted RT </span></span><br><span class="line">    self.Rt2 = np.hstack((R, T.reshape(<span class="number">3</span>, <span class="number">1</span>)))</span><br></pre></td></tr></table></figure><hr><h2 id="图片校正"><a href="#图片校正" class="headerlink" title="图片校正"></a>图片校正</h2><p>这里我提供了两种校正的算法。<br>一种是要用到之前的相机参数实现的校正，依赖于cv2.stereoRectify()<br>都是固有流程，原理见<em>Multiple View Geometry in computer vision</em><br>代码如下：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RectifyImg</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Rectify images using cv2.stereoRectify()</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        self.FE.all()</span><br><span class="line">    <span class="keyword">except</span> AttributeError:</span><br><span class="line">        self.EstimateFM() <span class="comment"># Using traditional method as default</span></span><br><span class="line"></span><br><span class="line">    self._Get_Essential_Matrix()</span><br><span class="line">    self._Get_R_T()</span><br><span class="line"></span><br><span class="line">    R = self.Rt2[:, :<span class="number">3</span>]</span><br><span class="line">    T = self.Rt2[:, <span class="number">3</span>]</span><br><span class="line">    <span class="comment">#perform the rectification</span></span><br><span class="line">    R1, R2, P1, P2, Q, roi1, roi2 = cv2.stereoRectify(self.Kl, self.dl,</span><br><span class="line">                                                        self.Kr, self.dr,</span><br><span class="line">                                                        self.imgl.shape[:<span class="number">2</span>],</span><br><span class="line">                                                        R, T, alpha=<span class="number">0</span>)</span><br><span class="line">    mapx1, mapy1 = cv2.initUndistortRectifyMap(self.Kl, self.dl, R1, P1,</span><br><span class="line">                                                [self.imgl.shape[<span class="number">0</span>],self.imgl.shape[<span class="number">1</span>]],</span><br><span class="line">                                                cv2.CV_32FC1)</span><br><span class="line">    mapx2, mapy2 = cv2.initUndistortRectifyMap(self.Kr, self.dr, R2, P2,</span><br><span class="line">                                                [self.imgl.shape[<span class="number">0</span>],self.imgl.shape[<span class="number">1</span>]],</span><br><span class="line">                                                cv2.CV_32FC1)</span><br><span class="line">    img_rect1 = cv2.remap(self.imgl, mapx1, mapy1, cv2.INTER_LINEAR)</span><br><span class="line">    img_rect2 = cv2.remap(self.imgr, mapx2, mapy2, cv2.INTER_LINEAR)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># save</span></span><br><span class="line">    cv2.imwrite(os.path.join(self.SavePath,<span class="string">&quot;RectedLeft.jpg&quot;</span>),img_rect1)</span><br><span class="line">    cv2.imwrite(os.path.join(self.SavePath,<span class="string">&quot;RectedRight.jpg&quot;</span>),img_rect2)</span><br></pre></td></tr></table></figure><p>另外一种是不需要相机内参的校正算法，依赖于cv2.stereoRectifyUncalibrated()，最后得到的是对应于两张图片的单应变换矩阵H1，H2。<br>接着使用cv2.remap函数就可以实现图片的校正。<br>代码如下：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">returnH1_H2</span>(<span class="params">points1,points2,F,size</span>):</span></span><br><span class="line">    p1=points1.reshape(len(points1)*<span class="number">2</span>,<span class="number">1</span>)<span class="comment">#stackoverflow上需要将(m,2)的点变为(m*2,1),因为不变在c++中会产生内存溢出</span></span><br><span class="line">    p2=points2.reshape(len(points2)*<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">    _,H1,H2=cv2.stereoRectifyUncalibrated(p1,p2,F,size) <span class="comment">#size是宽，高</span></span><br><span class="line">    <span class="keyword">return</span> H1,H2</span><br></pre></td></tr></table></figure><hr><h2 id="结果及说明"><a href="#结果及说明" class="headerlink" title="结果及说明"></a>结果及说明</h2><p>校正前：<br><img src="https://img-blog.csdnimg.cn/20200205151923511.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjczMDk5Nw==,size_16,color_FFFFFF,t_70" alt="Oringinal Left"><br><img src="https://img-blog.csdnimg.cn/20200205151909275.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjczMDk5Nw==,size_16,color_FFFFFF,t_70" alt="Oringinal Right"></p><p>校正后：<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL0Vhc29ueWVzaGVuZy9NYXJrZG93blBpY3R1cmVzL21hc3Rlci9SZWN0VW5jYWxpYkxlZnQuanBn?x-oss-process=image/format,png" alt="Rectifyed Left"><br><img src="https://img-blog.csdnimg.cn/20200205151838386.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjczMDk5Nw==,size_16,color_FFFFFF,t_70" alt="Rectifyed Right"></p><p>完整的代码在我的GitHub<a href="https://github.com/Easonyesheng/StereoCamera">项目</a>中有，记得给个Star。<br><em>中国人喜欢折中。如果你告诉他们不要白嫖，他们往往偏要看了用了不Star，如果你告诉他们给钱才能看才能用，他们就会Star来代替给钱。 – 周树人</em>  </p>]]></content>
    
    
    <summary type="html">基于opencv。</summary>
    
    
    
    <category term="博客" scheme="http://northpointer.xyz/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
    <category term="双目视觉" scheme="http://northpointer.xyz/tags/%E5%8F%8C%E7%9B%AE%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>基于python-opencv的相机标定原理及实现</title>
    <link href="http://northpointer.xyz/2020/08/13/Principle_of_Calibration_and_Implemention/"/>
    <id>http://northpointer.xyz/2020/08/13/Principle_of_Calibration_and_Implemention/</id>
    <published>2020-08-13T07:48:08.000Z</published>
    <updated>2020-10-10T13:22:06.554Z</updated>
    
    <content type="html"><![CDATA[<h1 id="单目相机标定原理及实现"><a href="#单目相机标定原理及实现" class="headerlink" title="单目相机标定原理及实现"></a>单目相机标定原理及实现</h1><pre><code>基于python-opencv。</code></pre><hr><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>整体的原理是张氏标定法，也就是用棋盘图片来完成标定的方法，由张正友教授在98年提出<a href="https://www.researchgate.net/publication/3193178_A_Flexible_New_Technique_for_Camera_Calibration">参考论文</a>。  </p><h3 id="手撕公式"><a href="#手撕公式" class="headerlink" title="手撕公式"></a>手撕公式</h3><pre><code>首先是从数学上推导标定的可行性，公式较多！  </code></pre><p>标定原理的最底层一定是从相机投影矩阵说起，因为标定的目的就是要求得相机的相关成像参数。那么对于针孔相机模型，其投影的过程可以通过数学模型来描述，不过在此之前，先<br>要介绍三个坐标系。  </p><ul><li>世界坐标系<br>  为三维世界建立的坐标系，点常用大写字母表示，齐次坐标坐标为$Q = [X,Y,Z,1]$。  </li><li>相机坐标系<br>  相机坐标系的原点是相机中心所在位置，同时Z轴是相机面朝的方向，点的坐标记为$q = [x,y,1]$。</li><li>图像坐标系<br>  相机生成图像平面为坐标系的XY平面，Z轴则朝向相机所指方向。一般与相机坐标系间存在一个尺度的变换（真实尺度到像素）。</li></ul><p>相机的投影模型描述的就是点从世界坐标系到图像坐标系的投影，$q = PQ$，P为相机投影矩阵。具体可以写成下图的形式：<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/Camera/ProjectionMatrix.png" alt="img"><br>图中M是相机的内参，包括x，y轴上的焦距和相机中心的偏移（相机制造时不可避免的偏移）。W是相机的外参，包括从世界坐标系到相机坐标系的旋转和平移矩阵。s则是相机坐标系到图像坐标系的尺度变换。<br>所以从理论上讲，如果我们要实现标定（明确相机内外参数），必然需要明确两个坐标系下的点坐标。<br>同时，注意到相机内参是不会随坐标系变化的，而相机外参则是对每个世界/图像坐标系都有不同的值。所以用同一坐标系定义下的多组点，或者用不同坐标系定义下的多组点都可以通过方程的堆积来求出相机的内外参。<br>但是张正友教授采用的是另一种更简单而高效的思路，就是从平面出发来标定。<br>图像平面自然是三维世界中的一个平面，而如果世界坐标系中的点也存在于一个平面，那么从平面到平面的点的投影就可以用另一种方式描述 –  单应变换（Homography Transformation）。<br>单应变换用一个3x3的矩阵来表示点到点的投影，这个矩阵可以通过四组点的对应来求。而为了使世界坐标系下的点坐标已知，这里的平面选择的是存在明确角点的国际象棋棋盘（设角点间的实际距离为1，第一个角点坐标为$(0,0,0)$，第二个为$(0,1,0)$，以此类推）。<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/Camera/PlaneTrans.png" alt="img"><br>此时，原先的点变换可以写为：$q = HQ$，而将棋盘设定为世界坐标系的XY平面，则棋盘上的点z坐标为0，所以有：<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/Camera/Ze0.png" alt="img"><br>将单应变换和投影变换联立，则有：<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/Camera/Homo2ProjM.png" alt="img"><br>所以相机的外参就可以用h表示为：<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/Camera/getExt.png" alt="img"><br>上式中$r_1.r_2$是旋转向量，所以满足正交且模长相等的约束：$r_1^Tr_2 = 0, |r_1| = |r_2|$<br>所以可以推出以下公式（称为两个约束）：<br>$h_1^T M^{-T}M^{-1}h_2=0 $<br>$h_1^T M^{-T}M^{-1}h_1 = h_2^T M^{-T}M^{-1}h_2 $<br>注意到两个式子中$M^{-T}M^{-1}$作为整体出现，所以可以用一个矩阵来代替：<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/Camera/BM.png" alt="img"><br>同时，B可以由相机内参唯一解出：<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/Camera/B2Int.png" alt="img"><br>注意到B是一个对称矩阵，所以有6个未知元素（但实际只有4个自由度），且约束中有$h_i^T B h_j$作为整体出现，重新排列B的元素构成向量b，则对任意的$h_i, h_j$有：<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/Camera/hBh.png" alt="img"><br>将两个约束带入，得到：<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/Camera/Vb.png" alt="img"><br>到这一步我们可以发现，$v_ij$是只和H有关的向量，而b是只和相机内参有关的向量，所以只要有足够多的H就可以通过这个方程组求出b，即求出相机内参。而对于单个H（一个朝向的棋盘）可以提供两个方程，对于b，其自由度为4，所以只要最少2个不同的H就可以求出b，这也是为什么实际操作时需要拍不同朝向的棋盘图片的原因。<br>相机内参求出如下：<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/Camera/getInt.png" alt="img"><br>求出相机内参后，对于特定棋盘的外参也可以求出：<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/Camera/getExt.png" alt="img"><br>而以上部分是没有考虑相机的两个畸变的（径向畸变和切向畸变），设$(x_p, y_p)$是点的真实位置，$(x_d, y_d)$是畸变后的位置，有替换公式如下：<br><img src="http://easonzhangyesheng.gitee.io/imgnorthpointer/Camera/distortion.png" alt="img"><br>相当于往原先的方程组中引入了5个未知参数，这样就需要更多的方程，即更多的棋盘图片。  </p><h3 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h3><p>以上的步骤只能得到一个无几何意义的初始解，为此zhang在论文中以该解为起点进行优化，优化目标是点的重投影误差，所以标定问题的难点就在于最后要解的是一个非线性最优化问题，zhang的论文中采用的是Levenberg-Marquardt算法。但是最优化问题常常会困在局部最优解上，同时不同的输入图片，也就是棋盘图片的位姿等因素，会对初始解造成影响，从而使结果困在不同的局部最优解上，导致标定的结果不同。<br>同时，真实相机的模型并不一定是小孔模型，最优化方法只能弥补一部分模型近似的误差。  </p><hr><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>实现是基于python-opencv。<br>棋盘图片需要选用长宽不等的（例如6x7），便于确认朝向，其次是棋盘的边缘最好留白，这样不会对找角点的函数造成影响。<br>具体流程是先用读入棋盘图片，然后用cv2.findChessboardCorners去寻找每张棋盘的角点，如果能够全部找到，就再去寻找亚像素坐标，用cv2.cornerSubPix。然后就是用cv2.calibrateCamera来标定，具体可以看代码中ModelCalibrator.py的Calibrator类的run方法。  </p><p>—–</p><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>求Star～ <a href="https://github.com/Easonyesheng/StereoCameraToolkit">传送门</a>  </p>]]></content>
    
    
    <summary type="html">原理详细，代码规范，走过路过，不容错过。</summary>
    
    
    
    <category term="博客" scheme="http://northpointer.xyz/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
    <category term="双目视觉" scheme="http://northpointer.xyz/tags/%E5%8F%8C%E7%9B%AE%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>2D射影几何与变换（1）</title>
    <link href="http://northpointer.xyz/2020/08/13/2DProjectGeometry_1/"/>
    <id>http://northpointer.xyz/2020/08/13/2DProjectGeometry_1/</id>
    <published>2020-08-13T07:48:08.000Z</published>
    <updated>2020-10-19T13:39:01.369Z</updated>
    
    <content type="html"><![CDATA[<h1 id="2D射影几何与变换（1）"><a href="#2D射影几何与变换（1）" class="headerlink" title="2D射影几何与变换（1）"></a>2D射影几何与变换（1）</h1><p>这篇主要介绍在2D射影空间中，点、直线与二次曲线的相关概念。<br>射影几何的核心在于从更高的维度看2维世界，从而可以得到很好的一些性质。  </p><hr><h2 id="点与直线"><a href="#点与直线" class="headerlink" title="点与直线"></a>点与直线</h2><h3 id="直线"><a href="#直线" class="headerlink" title="直线"></a>直线</h3><p>平面上的直线可以用$ax+by+c=0$来表示，那么就可以用向量$(a,b,c)^T$来表示一条直线，称为直线的齐次向量，因为$(a,b,c)^T$和$k(a,b,c)^T$表示同一个直线。**具有这种齐次性的向量称为齐次向量，$IR^3$中这样的向量集合（除去$(0,0,0)^T$）就构成是射影空间$IP^2$**。  </p><h3 id="点"><a href="#点" class="headerlink" title="点"></a>点</h3><p>对于点$(x,y)$，它在直线上的充要条件是$ax+by+c=0$，也就是$(a,b,c)(x,y,1)=0$，所以显然点在$IP^2$的齐次坐标为$(x,y,1)$，该坐标也满足齐次性，对于坐标$x=(x_1,x_2,x_3)$表示$IR^2$上的点$(x_1/x_3,x_2/x_3)$。  </p><h3 id="直线的交点-x-l-times-l’"><a href="#直线的交点-x-l-times-l’" class="headerlink" title="直线的交点$x = l \times l’$"></a>直线的交点$x = l \times l’$</h3><h3 id="点连成的直线-l-x-times-x’"><a href="#点连成的直线-l-x-times-x’" class="headerlink" title="点连成的直线$l = x \times x’$"></a>点连成的直线$l = x \times x’$</h3><h3 id="理想点"><a href="#理想点" class="headerlink" title="理想点"></a>理想点</h3><p>当我们考察两平行直线$ax+by+c=0, ax+by+c’=0$的交点时，得到交点为$(c’-c)(b,-a,0)$，显然该点不和二维平面空间上的任何点对应，而一般认为平行线交于无穷远点，所以将$x_3=0$的齐次坐标点看作无穷远点，也称为理想点。同时有$IP^2$的另一个解释，即将$x_3!=0$的齐次坐标对应的$IR^2$中的有限点的集合加入$x_3=0$的点，扩充的集合作为射影空间。可以验证，所有理想点构成一条直线$I=(0,0,1)^T$，即无穷远直线。<br>向量$(b,-a)$与直线$ax+by+c=0$的法线相切，所以是直线的方向，所以无穷远直线可以看作是所有直线方向的集合。</p><h2 id="从-IR-3-看-IP-2"><a href="#从-IR-3-看-IP-2" class="headerlink" title="从$IR^3$看$IP^2$"></a>从$IR^3$看$IP^2$</h2><p>$IP^2$中的点可以看作$IR^3$中过原点的射线，因为其满足齐次性，即在$IR^3$中不管长度，方向决定一个点，同样，$IP^2$中的线可以看作$IR^3$中过原点的平面。  </p><h2 id="自由度"><a href="#自由度" class="headerlink" title="自由度"></a>自由度</h2><p>$IP^2$中元素的自由度往往是其向量维度减1，因为要满足齐次性，所以是由比率来确定唯一性的。  </p><h2 id="对偶性"><a href="#对偶性" class="headerlink" title="对偶性"></a>对偶性</h2><p>在$IP^2$中，任何定理都有一个对偶定理，可以通过互换点和线的角色来得到。如上文中直线的交点和点连成的直线。  </p><h2 id="二次曲线"><a href="#二次曲线" class="headerlink" title="二次曲线"></a>二次曲线</h2><p>在欧式几何中，二次曲线是不同平面与圆锥的截线，方程为$ax^2+bxy+cy^2+dx+ey+f=0$，齐次化（$x-&gt;x_1/x_3, y-&gt;x_2/x_3$）得到$ax_1^2+bx_1x_2+cx_2^2+dx_1x_3+ex_2x_3+fx_3^2=0$，可以写成矩阵$x^TCx=0$，其中<br>$$<br>C=\left[<br>    \begin{matrix}<br>    a &amp; b/2 &amp; d/2 \<br>    b/2 &amp; c &amp; e/2 \<br>    d/2 &amp; c/2 &amp; f<br>    \end{matrix}<br>    \right]<br>$$<br>所以每一个对称矩阵都是一个二次曲线，因为其自由度为5，可以通过5个点来确定。  </p><h3 id="与二次曲线相切于x的直线满足-I-Cx"><a href="#与二次曲线相切于x的直线满足-I-Cx" class="headerlink" title="与二次曲线相切于x的直线满足$I = Cx$"></a>与二次曲线相切于x的直线满足$I = Cx$</h3><h3 id="对偶二次曲线"><a href="#对偶二次曲线" class="headerlink" title="对偶二次曲线"></a>对偶二次曲线</h3><p>由对偶性，可以得到用直线定义的二次曲线$l^T C^{<em>} l=0$，其中$C^{</em>}$是C的伴随矩阵，对于可逆对称阵，$C^{*}=C^{-1}$，对偶二次曲线是由直线与二次曲线相切得到的，也就是说它的元素是与二次曲线相切的直线，而二次曲线的元素是点。对偶二次曲线是直线族，而对应的二次曲线是这些直线的包络。  </p><h3 id="退化二次曲线"><a href="#退化二次曲线" class="headerlink" title="退化二次曲线"></a>退化二次曲线</h3><p>如果C不满秩，则二次曲线是退化的，为两条直线（rank2）或一条重线（rank1）。 </p>]]></content>
    
    
    <summary type="html">计算机视觉中的多视图几何笔记</summary>
    
    
    
    <category term="博客" scheme="http://northpointer.xyz/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
    <category term="双目视觉" scheme="http://northpointer.xyz/tags/%E5%8F%8C%E7%9B%AE%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>数字图像处理总结4</title>
    <link href="http://northpointer.xyz/2020/08/13/DIP4/"/>
    <id>http://northpointer.xyz/2020/08/13/DIP4/</id>
    <published>2020-08-13T06:43:46.000Z</published>
    <updated>2020-08-19T11:53:31.461Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图像处理总结4"><a href="#图像处理总结4" class="headerlink" title="图像处理总结4"></a>图像处理总结4</h1><p>灰度形态学<br>之前说过：<br><strong>形态学起源于法国巴黎高等矿业学院，因为人家是搞地质的。</strong><br><strong>可见一斑，形态学的精要就是在于将图像看作是等高线组成的地形图，他的基本操作就是动土，平高填低等等。</strong><br>之前的二值形态学针对的是二值图片，在地形上，就像是现代的建筑，拔地而起没有一定的梯度。<br>而这篇是针对灰度图片的形态学，这时的图片就更像自然界的地貌，像山川，像河谷。  </p><hr><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><p>基本操作也是腐蚀和膨胀。<br><strong>Dilation（膨胀）：</strong><br>先看公式：$F\bigoplus k=max_{a,b\in k}{F(x-a,y-b)+k(a,b)}$<br>同样有两种算法。<br>1）平移<br>对原图先加上SE对应部分的值，再向SE对应的位置平移，然后取最大值。<br>比如，现在有一个一维的SE: [1,2,3], 中心点定为第一个点，就是“1”。<br>那么首先对应于“1”，则是原图位置不变加1.<br>对于“2”，原图灰度值加2，向右平移一位。<br>对于“3”，原图灰度值加3，向右平移两位。<br>最后将三张图重合，并在原图区域取最大值，作为结果图。<br>2）盖章<br>SE在原图上滑动，每个滑动的位置，SE与原图相加，中心点取SE范围内最大值作为结果。<br>这个程序上好实现。<br>==直观上，就是原图亮的地方会更亮（整体亮度增加），且范围扩大。==<br><strong>Erosion（腐蚀）：</strong><br>公式：$F\bigodot k=min_{a,b \in k}{F(x-a,y-b)-k(a,b)}$<br>与膨胀不同的就是与SE相减，取最小值。<br>==直观上就是，暗的地方会更暗（整体亮度减小）==<br><strong>Open（开）：</strong><br>先腐蚀后膨胀。<br>==直观上就是消除了原图上比SE小的亮处。==<br><strong>Close（闭）：</strong><br>先膨胀后腐蚀。<br>==直观上与Open相反，消除了暗处。==  </p><hr><h2 id="组合算法"><a href="#组合算法" class="headerlink" title="组合算法"></a>组合算法</h2><p><strong>TopHat Transform:</strong><br>高帽变换分为白色高帽变换和黑色高帽变换。<br>白色是指将灰度Close过的图和原图相减。<br>黑色是指将原图与灰度Open过的图相减。<br>总之就是留下了灰度形态学操作突出的部分。  </p><p><strong>Grayscale Reconstruction:</strong><br>灰度重建分为OBR和CBR。<br>一个是针对Open后的图片重建。<br>一个是针对Close后的图片重建。<br>重建算法就是不断膨胀，但每次膨胀后与原图比较，将灰度值高于原图处的点置为原图的灰度值。<br>意思就是不能超过原图的灰度值。重复膨胀直到稳定。<br>==类似二值里面的Conditional Dilation==</p><p><strong>梯度：</strong><br>灰度形态学也有梯度，和二值里面的相同。  </p><hr><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>这里的算法都可以在我的GitHub里的<a href="https://github.com/Easonyesheng/DIP_GUI">项目</a>里找到<br>包括灰度形态学基本操作（D，E，O，C），以及部分组合算法。<br>该项目是一个包括阈值分割、卷积滤波、形态学和灰度形态学的数字图像处理算法集合<br>用法见readme文件。<br>且带有GUI，有很好的演示效果<br>觉得可以别忘了star哦  </p>]]></content>
    
    
    <summary type="html">灰度形态学。</summary>
    
    
    
    <category term="博客" scheme="http://northpointer.xyz/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
    <category term="数字图像处理" scheme="http://northpointer.xyz/tags/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>数字图像处理总结3</title>
    <link href="http://northpointer.xyz/2020/08/13/DIP3/"/>
    <id>http://northpointer.xyz/2020/08/13/DIP3/</id>
    <published>2020-08-13T06:42:02.000Z</published>
    <updated>2020-08-19T11:53:26.377Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图像处理总结3"><a href="#图像处理总结3" class="headerlink" title="图像处理总结3"></a>图像处理总结3</h1><p>二值形态学<br><strong>形态学起源于法国巴黎高等矿业学院，因为人家是搞地质的。</strong><br><strong>可见一斑，形态学的精要就是在于将图像看作是等高线组成的地形图，他的基本操作就是动土，平高填低等等。</strong><br>当然，这是哲学层面的抽象概念，太玄，还是整点实际的。<br>==需要注意的是，这里处理的图片都是二值图片（0 or 1）。==  </p><hr><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><p><strong>结构元素（SE）</strong><br>就是你要动土的那块地方多大，以及要怎么动土。  </p><p><strong>腐蚀（Erosion）：</strong><br>先看公式：$E(F,k)=F\bigodot k=\bigcap ({ a+b|a \in F})$<br>F：原图，k：SE<br>这里就有两种看法。<br>第一种，核心：<em>平移</em><br>原图按照结构元素平移，在原图大小里取交集。<br>这里要关心SE的中心在哪里，SE里有1的地方就是要平移的方向。<br>第二种，核心：<em>盖章</em><br>SE在原图上平移，只有能够完全包住SE的位置的点置1，其他置0.<br>==腐蚀能够让抹去图片上的小亮点，打断小连接，总之会让图片中亮处整体缩小。==  </p><p><strong>膨胀（Dilation）：</strong><br>公式：$D(F,k)=F\bigoplus k=\bigcup ({ a-b|a\in F})$<br>同样的两种看法，平移的时候是取并集，盖章的时候是只要SE有于原图区域中重合的地方，中心点就置1。<br>==膨胀能够让图片亮处整体扩大。==  </p><p><strong>开（Open）：</strong><br>先腐蚀后膨胀。<br>==可以抹去图片中size小于SE的点和连接，而不会对其他的部分造成大的破坏。==  </p><p><strong>闭（Close）：</strong><br>先膨胀再腐蚀。<br>==可以填补小于SE size的空洞，弥合size小于SE的裂缝，而不会对其他的部分造成大的破坏。==  </p><hr><h2 id="组合算法"><a href="#组合算法" class="headerlink" title="组合算法"></a>组合算法</h2><h3 id="Distance-Transform-距离变换"><a href="#Distance-Transform-距离变换" class="headerlink" title="Distance Transform(距离变换)"></a>Distance Transform(距离变换)</h3><p>这种操作就是用一个flat的SE，给原图作连续的腐蚀，原图上每个点的值替换为n(最后拉伸到0-255)，这个点在第n次腐蚀时消失。<br>直观感觉就是将图片中间的点变亮，周围的点变暗，此时二值图变为灰度图。  </p><h3 id="Skeleton-骨架提取"><a href="#Skeleton-骨架提取" class="headerlink" title="Skeleton(骨架提取)"></a>Skeleton(骨架提取)</h3><p>这种操作直观上是提取出原图形状的中心线，即骨架。<br>是有公式的：<br>$Result = \bigcup S_i(F)$<br>$S_i(F) = F \bigodot r_ik - {(F \bigodot r_ik)\circ r_ik}$</p><h3 id="Edge-形态学边缘"><a href="#Edge-形态学边缘" class="headerlink" title="Edge(形态学边缘)"></a>Edge(形态学边缘)</h3><p>用原图减去腐蚀后的图，或者用膨胀图减去原图，或者用膨胀图减去腐蚀图。<br>都可以得到原图物体的边缘。  </p><h3 id="Conditional-Dilation-条件膨胀"><a href="#Conditional-Dilation-条件膨胀" class="headerlink" title="Conditional Dilation(条件膨胀)"></a>Conditional Dilation(条件膨胀)</h3><p>这是一个很有意思，也很有用的算法。<br>有什么用呢？–==可以通过这个算法去分离原图中两个不相连的part。==<br>条件是只要知道一个物体的一个点在哪里。<br>然后就对这个点作连续膨胀，每次膨胀都和原图作逻辑操作中的“与“操作。<br>这样就可以保证膨胀出的内容不会超过原图的内容。<br>而当每次”与“之后稳定了，就分出了想要的part。  </p><hr><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>这里的算法都可以在我的GitHub里的<a href="https://github.com/Easonyesheng/DIP_GUI">项目</a>里找到<br>该项目是一个包括阈值分割、卷积滤波、形态学和灰度形态学的数字图像处理算法集合<br>用法见readme文件。<br>且带有GUI，有很好的演示效果<br>觉得可以别忘了star哦  </p>]]></content>
    
    
    <summary type="html">二值形态学。</summary>
    
    
    
    <category term="博客" scheme="http://northpointer.xyz/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
    <category term="数字图像处理" scheme="http://northpointer.xyz/tags/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>数字图像处理总结2</title>
    <link href="http://northpointer.xyz/2020/08/13/DIP2/"/>
    <id>http://northpointer.xyz/2020/08/13/DIP2/</id>
    <published>2020-08-13T06:41:07.000Z</published>
    <updated>2020-08-19T11:53:15.855Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数字图像处理总结2"><a href="#数字图像处理总结2" class="headerlink" title="数字图像处理总结2"></a>数字图像处理总结2</h1><p>卷积运算和图像滤波</p><hr><h2 id="卷积运算"><a href="#卷积运算" class="headerlink" title="卷积运算"></a>卷积运算</h2><p>对图像的操纵分为点操作，代数操作，几何操作和领域操作。<br>卷积运算就是领域操作中的一个，而提到卷积，就不得不说他的姊妹correlation。<br>话不多说，先看公式：<br>convolution : $f*w=\sum_{(a,b)\in w}f(x-a,y-b)w(a,b)$<br>correlation : $f \bigotimes w=\sum _{(a,b) \in w}f(x+a,y+b)w(a,b) $<br>f是原图，k是kernel。a,b是kernel中的点坐标，默认kernel的中心坐标为（0，0）。<br>可以看出两者的区别主要是原图的坐标变换是+ or -<br>而且因为convolution是 - ，在代码中具体运算时，需要将kernel旋转$180^\circ$。<br>一句话描述卷积的过程：对一个以某点为中心，kernel大小的原图区域，对应乘上旋转过的kernel的元素并求和，结果来作为kernel中心那个像素的新的灰度值，对图中所有点作该操作，就是一个卷积。  </p><hr><h2 id="图像滤波"><a href="#图像滤波" class="headerlink" title="图像滤波"></a>图像滤波</h2><p>图像滤波就是用不同的kernel进行卷积操作。<br>根据kernel的不同，又分为很多目的不同的操作，如边缘检测、图像平滑等。<br>下面就举几个常见的例子：  </p><hr><h3 id="Edge-Detection"><a href="#Edge-Detection" class="headerlink" title="Edge Detection"></a>Edge Detection</h3><p><strong>典型算子：</strong><br>Roberts：<br>$$<br>\left |<br> \begin{matrix}<br>  -1 &amp; 0 \<br>  0 &amp; 1 \<br> \end{matrix}<br>\right |<br>$$<br>$$<br>\left |<br> \begin{matrix}<br>  0 &amp; -1 \<br>  1 &amp; 0 \<br> \end{matrix}<br>\right |<br>$$</p><p>Prewitt:<br>$$<br>\left |<br> \begin{matrix}<br>  -1 &amp; -1 &amp; -1 \<br>  0 &amp; 0 &amp; 0 \<br>  1 &amp; 1 &amp; 1 \<br> \end{matrix}<br>\right |<br>$$<br>$$<br>\left |<br> \begin{matrix}<br>  -1 &amp; 0 &amp; 1 \<br>  -1 &amp; 0 &amp; 1 \<br>  -1 &amp; 0 &amp; 1 \<br> \end{matrix}<br>\right |<br>$$<br>Sobel:<br>$$<br>\left |<br> \begin{matrix}<br>  -1 &amp; -2 &amp; -1 \<br>  0 &amp; 0 &amp; 0 \<br>  1 &amp; 2 &amp; 1 \<br> \end{matrix}<br>\right |<br>$$<br>$$<br>\left |<br> \begin{matrix}<br>  -1 &amp; 0 &amp; 1 \<br>  -2 &amp; 0 &amp; 2 \<br>  -1 &amp; 0 &amp; 1 \<br> \end{matrix}<br>\right |<br>$$<br>通过观察可以看出，这三个算子都强调不同像素间的差，其实就是离散形式的导数，所以这三个算子都是在求图片的梯度。<br>而求梯度，就是突出变化，也就是能够得到图片的中像素值变化剧烈的地方，也就是边缘。  </p><hr><h3 id="图像平滑"><a href="#图像平滑" class="headerlink" title="图像平滑"></a>图像平滑</h3><p><strong>典型算子：</strong><br>平均滤波：<br>$$<br>\left [<br> \begin{matrix}<br>  1/9 &amp; 1/9 &amp; 1/9 \<br>  1/9 &amp; 1/9 &amp; 1/9 \<br>  1/9 &amp; 1/9 &amp; 1/9 \<br> \end{matrix}<br>\right ]<br>$$<br>中值滤波：<br>这个算子不是寻常的计算算子，而是将3x3范围内的中值作为中心的灰度值。<br>是一种统计学算子。<br>Gaussian:<br>$G(x,y)={1/{2\pi \sigma^2}} \times e^{-{x^2+y^2}/{2\sigma^2}}$<br>这个式子指的是高斯滤波器对应位置的值，其中一个关键元素是$\sigma$，是方差，在滤波时一般自己选定，他决定了高斯分布的峰高和宽（越大越矮、宽）  </p><hr><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>实现时主要是依赖于opencv的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv2.filter2D(img,<span class="number">-1</span>,filter)</span><br></pre></td></tr></table></figure><p>这里的算法都可以在我的GitHub里的<a href="https://github.com/Easonyesheng/DIP_GUI">项目</a>里找到<br>该项目是一个包括阈值分割、卷积滤波、形态学和灰度形态学的数字图像处理算法集合<br>且带有GUI，有很好的演示效果<br>觉得可以别忘了star哦  </p>]]></content>
    
    
    <summary type="html">卷积运算和图像滤波。</summary>
    
    
    
    <category term="博客" scheme="http://northpointer.xyz/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
    <category term="数字图像处理" scheme="http://northpointer.xyz/tags/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>数字图像处理总结1</title>
    <link href="http://northpointer.xyz/2020/08/11/DIP-1/"/>
    <id>http://northpointer.xyz/2020/08/11/DIP-1/</id>
    <published>2020-08-11T06:01:12.000Z</published>
    <updated>2020-08-19T11:53:21.528Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数字图像处理总结一"><a href="#数字图像处理总结一" class="headerlink" title="数字图像处理总结一"></a>数字图像处理总结一</h1><p>图像概念 &amp; 阈值分割  </p><h2 id="图像"><a href="#图像" class="headerlink" title="图像"></a>图像</h2><p>数字图像描述的是一个二元函数f(x,y)，将图像的坐标和灰度值联系起来。<br>有三种图片，但本质都是灰度图像，像素值也称为灰度值。<br>图像种类分为：<br>    二值图像 – gray level = 0 or 255/1<br>    灰度图像 – gray level in [0,255]<br>    彩色图像 – 三个通道的灰度图片<br>    伪彩图像、高维图片 – 不作详细介绍  </p><h2 id="阈值分割算法"><a href="#阈值分割算法" class="headerlink" title="阈值分割算法"></a>阈值分割算法</h2><p>阈值分割就是利用图片灰度的分布规律，来分割出图片中不同的物体。<br>而每个物体在图片中的都是一个个的正态分布<br>所以阈值分割就需要找到一个阈值来分出几个正态分布<br>而要找到那个阈值，就需要引入一个概念 – 灰度直方图  </p><h3 id="灰度直方图"><a href="#灰度直方图" class="headerlink" title="灰度直方图"></a>灰度直方图</h3><p>就是统计图片中的灰度分布。<br>横坐标是0～255的灰度值，纵坐标是每个灰度值在图片中的出现的个数<br>每一个物体都有在一段灰度值中的分布，可以看出图片中物体的分布<br>找到那个阈值之后，是可以将阈值之下的置为0，之上的置为最高值。<br>但每张图都有不同的阈值，所以需要一个算法，可以针对不同的图，自动找到那个可以分割出的阈值<br>即算法的普适性  </p><h3 id="OTSU"><a href="#OTSU" class="headerlink" title="OTSU"></a>OTSU</h3><p>OTSU是一个可以自动找到分割阈值的算法<br>其核心思想是：<br><strong>将所有像素通过一个阈值分为前景和背景，然后用一个方差来描述两个类距离图像中心的距离的加和，这个方差最大时，两个类分得最开，对应用的阈值最优</strong><br>算法描述：<br>先得到灰度直方图<br>对0-255的每个灰度值，来计算方差<br>方差公式：<br>$$<br>\sigma^2=\omega_0 \omega_1(\mu_1-\mu_0)^2<br>$$<br>得到最大方差对应的阈值作为分割阈值  </p><h3 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h3><p>熵分割也是一种可以自己找到分割需要的阈值的算法<br>熵描述的是信息量<br>而阈值分割问题，其实就是将图片分为前景和背景<br>如果前景信息量大的同时，背景的信息量也很大，那么就说明分割的比较好<br>所以核心思想就是：<br>$$<br>Maximum(H=H_W+H_B)<br>$$  </p><p>算法描述：<br>求出灰度直方图<br>对每个像素值，计算前景背景熵之和<br>得到熵最大的那个阈值作为最终的阈值  </p><p>熵:<br>$$<br>H_b=-\sum^{0 \to t}{p_i log(p_i)}<br>$$  </p><p>$$<br>H_W=-\sum^{t+1 \to 255}{p_ilog(p_i)}<br>$$  </p><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>这里的两种阈值算法都可以在我的GitHub提交的<a href="https://github.com/Easonyesheng/DIP_GUI">项目</a>里找到<br>该项目是一个包括阈值分割、卷积滤波、形态学和灰度形态学的数字图像处理算法集合<br>且带有GUI，有很好的演示效果<br>觉得可以别忘了star哦  </p>]]></content>
    
    
    <summary type="html">图像概念与阈值分割算法。</summary>
    
    
    
    <category term="博客" scheme="http://northpointer.xyz/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
    <category term="数字图像处理" scheme="http://northpointer.xyz/tags/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
</feed>
